\documentclass[10pt]{article}
\usepackage[margin=1.3cm]{geometry}

% Packages
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{bbm} 
\usepackage{dutchcal} % [dutchcal, calrsfs, pzzcal] calligraphic fonts
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[tracking]{microtype}

% Palatino for text goes well with Euler
\usepackage[sc,osf]{mathpazo}   % With old-style figures and real smallcaps.
\linespread{1.025}              % Palatino leads a little more leading

% Euler for math and numbers
\usepackage[euler-digits,small]{eulervm}

% Command initialization
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\graphicspath{{./images/}}

% Custom Commands
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\bp}[1]{\left({#1}\right)}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\1}[1]{\mathbbm{1}_{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\nck}[2]{{#1\choose#2}}
\newcommand{\pc}[1]{\pazocal{#1}}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand*{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand*{\ceil}[1]{\left\lceil#1\right\rceil}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\sgm}{sgm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}
    \begin{center}
        {\bf\large{MATH 857: STATISTICAL LEARNING II}}
        \smallskip
        \hrule
        \smallskip
        {\bf Assignment 4} \hfill {\bf Connor Braun} \hfill {\bf 2024-04-03}
    \end{center}
    {\bf Problem 2}\\[5pt]
    Show that the kernel density estimator
    \[\hat{f}_n(x)=\frac{1}{n}\sum_{i=1}^n\frac{1}{h}K\bp{\frac{x-X_i}{h}}\]
    integrates to $1$, where $K$ satisfies $\int_{\mbb{R}} Kdx=1$.\\[5pt]
    {\bf Solution}\hspace{5pt} The claim can be verified directly:
    \begin{align*}
        \int_{\mbb{R}}\hat{f}_n(x)dx=\int_{-\infty}^\infty\frac{1}{n}\sum_{i=1}^n\frac{1}{h}K\bp{\frac{x-X_i}{h}}dx&=\frac{1}{n}\sum_{i=1}^n\frac{1}{h}\int_{-\infty}^\infty K\bp{\frac{x-X_i}{h}}dx\\
        &=\frac{1}{n}\sum_{i=1}^n\frac{1}{h}\int_{-\infty}^\infty hK\bp{u}du\tag{under change of variables: $u=\tfrac{x-X_i}{h}$, $\forall i$}\\
        &=\frac{1}{n}\sum_{i=1}^n1\tag{since $\int Kdx=1$}\\
        &=1
    \end{align*}
    and we are done.\hfill{$\qed$}\\[5pt]
    {\bf Problem 3}\\[5pt]
    Let $\hat{f}_h$ denote the kernel density estimator with bandwith parameter $h$. Define
    \begin{align*}
        J(h):=\int\hat{f}_h^2(x)dx-2\int\hat{f}_h(x)f(x)dx
    \end{align*}
    and
    \begin{align*}
        \hat{J}(h)=\int\hat{f}^2_h(x)dx-\frac{2}{n}\sum_{i=1}^nf_{-i,h}(X_i),
    \end{align*}
    where $\hat{f}_{-i,h}$ is the kernel density estimator without the $i$th observation. Show that $\E(\hat{J}(h))=\E(J(h))$.\\[5pt]
    {\bf Solution}\hspace{5pt} Let $X=(X_1,\dots,X_n)$ be an $\mbb{R}^n$-valued random variable with $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}f$. Then we have
    \begin{align*}
        \frac{1}{2}\E_X\bp{J(h)-\hat{J}(h)}=\E_X\bp{\frac{1}{2}\bp{-2\int_\mbb{R}\hat{f}_h(x)f(x)dx+\frac{2}{n}\sum_{i=1}^nf_{-i,h}(X_i)}}=-\E_X\bp{\int_\mbb{R}\hat{f}_h(x)f(x)dx}+\E_X\bp{\frac{1}{n}\sum_{i=1}^nf_{-i,h}(X_i)}.\tag{1}
    \end{align*}
    Whence we compute each of the terms in the last expression individually. First, denoting $\mc{L}(X)$ the law of $X$, 
    \begin{align*}
        \E_X\bp{\int_\mbb{R}\hat{f}_h(x)f(x)dx}&=\E_X\bp{\E_X\bp{\hat{f}_h(X_1)}}\tag{since $X_1,\dots,X_n$ are i.i.d.}\\
        &=\E_X\bp{\hat{f}_h(X_1)}\\
        &=\frac{1}{nh}\sum_{i=1}^n\int_{\mbb{R}^n}K\bp{\frac{X_1-X_i}{h}}\mc{L}(X)(dX)\\
        &=\frac{1}{nh}\sum_{i=1}^n\int_\mbb{R}\int_{\mbb{R}}K\bp{\frac{x-y}{h}}f(x)f(y)dxdy\tag{2}\\
        &=\frac{1}{h}\int_\mbb{R}\int_\mbb{R}K\bp{\frac{x-y}{h}}f(x)f(y)dxdy\tag{3}
    \end{align*}
    where (2) holds by Fubini's theorem, since $\mc{L}(X)=\mc{L}(X_1)\times\mc{L}(X_2)\times\cdots\mc{L}(X_n)$ and each of the $\mc{L}(X_i)$ admit the same density $f$ with respect to the Lebesgue measure (the first equality in the chain holds by a similar reasoning applied in reverse).\\[5pt]
    Next, with the same tricks, we compute
    \begin{align*}
        \E_X\bp{\frac{1}{n}\sum_{i=1}^nf_{-i,h}(X_i)}&=\E_X\bp{\frac{1}{n(n-1)h}\sum_{i=1}^n\sum_{j\neq i}K\bp{\frac{X_i-X_j}{h}}}\\
        &=\frac{1}{n(n-1)h}\sum_{i=1}^n\sum_{j\neq i}\int_{\mbb{R}^n}K\bp{\frac{X_i-X_j}{h}}\mc{L}(X)(dX)\\
        &=\frac{1}{n(n-1)h}\sum_{i=1}^n\sum_{j\neq i}\int_\mbb{R}\int_\mbb{R}K\bp{\frac{x-y}{h}}f(x)f(y)dxdy\\
        &=\frac{(n-1)}{n(n-1)h}\sum_{i=1}^n\int_\mbb{R}\int_\mbb{R}K\bp{\frac{x-y}{h}}f(x)f(y)dxdy\\
        &=\frac{n(n-1)}{n(n-1)h}\int_\mbb{R}\int_\mbb{R}K\bp{\frac{x-y}{h}}f(x)f(y)dxdy\\
        &=\frac{1}{h}\int_\mbb{R}\int_\mbb{R}K\bp{\frac{x-y}{h}}f(x)f(y)dxdy.\tag{4}
    \end{align*} 
    Observing that (3) and (4) are identical, and denoting this common expression with $\tilde{K}(h)$, we can return to (1) to find that
    \begin{align*}
        \frac{1}{2}\E\bp{J(h)-\hat{J}(h)}=\E_X\bp{\frac{1}{n}\sum_{i=1}^nf_{-i,h}(X_i)}-\E_X\bp{\int_\mbb{R}\hat{f}_h(x)f(x)dx}=\tilde{K}(h)-\tilde{K}(h)=0
    \end{align*}
    from which we obtain $\E(J(h))=\E(\hat{J}(h))$, as desired.\hfill{$\qed$}\\[5pt]
    {\bf Problem 4}\\[5pt]
    Consider a mixture model of two exponential distributions:
    \[f(x)=\alpha\lambda_1e^{-\lambda_1x}+(1-\alpha)\lambda_2e^{-\lambda_2x},x>0.\]
    {\bf a)}\hspace{5pt} Recall the latent class membership view on the mixture model. That is, there is an underlying latent variable $\delta\sim\text{Bernoulli}(\alpha)$ such that if $\delta=1$, $Y$ is
    generated from the density $\lambda_1e^{-\lambda_1y}$ and if $\delta=0$, $Y$ is generated from the density $\lambda_2e^{-\lambda_2y}$. We can then regard the complete data to be $\mathcal{D}_c=\{(y_i,\delta_i)\}_{i=1}^n$
    while the observed data are just $\mathcal{D}=\{y_i\}_{i=1}^n$. Write down the complete data likelihood and log-likelihood functions.\\[5pt]
    {\bf Solution}\hspace{5pt} We take $\theta=(\alpha, \lambda_1,\lambda_2)$ to be a parameterization of the joint distribution of $(Y,\delta)$, taking $\mc{D}_c$ to be an i.i.d. sample thereof. The marginal distribution of $Y$ can be written
    \begin{align*}
        f(y|\theta)=P(Y=y|\theta)=\sum_{k=0,1}P(Y=y,\delta=k|\theta)&=\sum_{k=0,1}P(Y=y|\delta=k,\theta)P(\delta=k|\theta)\\
        &=\lambda_1e^{-\lambda_1y}P(\delta=1|\theta)+\lambda_2e^{-\lambda_2y}P(\delta=0|\theta)\\
        &=\lambda_1e^{-\lambda_1y}\alpha+\lambda_2e^{-\lambda_2y}(1-\alpha)
    \end{align*}
    revealing the form of the joint density of $(Y,\delta)$:
    \[P(Y=y,\delta=0|\theta)=(1-\alpha)\lambda_2e^{-\lambda_2y},\quad P(Y=y,\delta=1|\theta)=\alpha \lambda_1e^{-\lambda_1y}.\]
    which renders clear the form of the likelihood function for the complete data:
    \begin{align*}
        L(\theta|\mc{D}_c)=\prod_{i=1}^nP(Y=y_i,\delta=\delta_i)=\prod_{i=1}^n\left[\alpha\lambda_1e^{-\lambda_1y_i}\right]^{\delta_i}\left[(1-\alpha)\lambda_2e^{-\lambda_2y_i}\right]^{(1-\delta_i)}
    \end{align*}
    with corresponding log-likelihood,
    \begin{align*}
        \ell(\theta|\mc{D}_c)=\sum_{i=1}^n\delta_i\log(\alpha)+\delta_i\log(\lambda_1)+(1-\delta_i)\log(1-\alpha)+(1-\delta_i)\log(\lambda_2)-\delta_i\lambda_1y_i-(1-\delta_i)\lambda_2y_i.\tag*{$\qed$}
    \end{align*}
    {\bf b)}\hspace{5pt}Derive the E-step and M-step of the EM algorithm for finding the MLE of the above mixture model.\\[5pt]
    {\bf Solution}\hspace{5pt}For the E-step, we simply compute the expected complete data log-likelihood given a parameterization $\theta^\prime=(\alpha^\prime,\lambda_1^\prime,\lambda_2^\prime)$ and the observed data $y=(y_1,\dots,y_n)$,
    \begin{align*}
        Q(\theta|\theta^\prime)=\E(\ell(\theta|\mc{D}_c)|y,\theta^\prime)=\sum_{i=1}^n\E(\delta_i|y,\theta^\prime)\left[\log(\alpha)+\log(\lambda_1)-\lambda_1y_i\right]+(1-\E(\delta_i|y,\theta^\prime))\left[\log(1-\alpha)+\log(\lambda_2)-\lambda_2y_i\right].
    \end{align*}
    For this, we need to compute the $\E(\delta_i|y,\theta^\prime)$ term:
    \begin{align*}
        \E(\delta|y,\theta^\prime)=\E(\delta_i|y_i,\theta^\prime)&=\frac{P(y_i|\delta_i,\theta^\prime)P(\delta_i|\theta^\prime)}{P(y_i|\theta^\prime)}\\
        &=\frac{P(y_i|\delta_i,\theta^\prime)P(\delta_i|\theta^\prime)}{P(y_i,\delta_i=0|\theta^\prime)+P(y_i,\delta_i=1|\theta^\prime)}\\
        &=\frac{P(y_i|\delta_i,\theta^\prime)P(\delta_i|\theta^\prime)}{P(y_i|\delta_i=0,\theta^\prime)P(\delta_i=0|\theta^\prime)+P(y_i|\delta_i=1,\theta^\prime)P(\delta_i=1|\theta^\prime)}\\
        &=\frac{(\alpha^\prime\lambda_1^\prime e^{-\lambda_1^\prime y_i})^{\delta_i}((1-\alpha^\prime)\lambda_2^\prime e^{-\lambda_2^\prime y_i})^{1-\delta_i}}{\alpha^\prime\lambda_1^\prime e^{-\lambda_1^\prime y_i}+(1-\alpha^\prime)\lambda_2^\prime e^{-\lambda_2^\prime y_i}}
    \end{align*}
    and we denote this expression with $\overline{\delta}_i=\E(\delta_i|y,\theta^\prime)$. Thus, $Q$ is expressed entirely in terms of observed data and known variables, and we have
    \[Q(\theta|\theta^\prime)=\sum_{i=1}^n\overline{\delta}_i\left[\log(\alpha)+\log(\lambda_1)-\lambda_1y_i\right]+(1-\overline{\delta}_i)\left[\log(1-\alpha)+\log(\lambda_2)-\lambda_2y_i\right].\]
    For the M-step, we derive the parameter updates by maximizing the $Q$ function over $\theta=(\alpha,\lambda_1,\lambda_2)$. Suppose that $\hat{\theta}=(\hat{\alpha},\hat{\lambda}_1,\hat{\lambda}_2)$ is a critical point such that $\nabla Q(\hat{\theta}|\theta^\prime)=0$. Then we compute
    \begin{align*}
        \frac{\partial Q(\theta|\theta^\prime)}{\partial\alpha}&=\sum_{i=1}^n\frac{\overline{\delta}_i}{\alpha}+\frac{\overline{\delta}_i-1}{1-\alpha}=\sum_{i=1}^n\frac{\overline{\delta}_i-\alpha}{\alpha(1-\alpha)}=\frac{1}{\alpha(1-\alpha)}\bp{\bp{\sum_{i=1}^n\overline{\delta}_i}-n\alpha}\\
        &\Rightarrow\quad \sum_{i=1}^n\overline{\delta}_i=n\hat{\alpha}\quad\Rightarrow\quad \hat{\alpha}=\frac{1}{n}\sum_{i=1}^n\overline{\delta}_i.
    \end{align*}
    Similarly, we find that
    \begin{align*}
        \frac{\partial Q(\theta|\theta^\prime)}{\partial\lambda_1}=\sum_{i=1}^n\frac{\overline{\delta}_i}{\lambda_1}-\overline{\delta}_iy_i\quad\Rightarrow\quad\sum_{i=1}^n\frac{\overline{\delta}_i}{\hat{\lambda}_1}=\sum_{i=1}^n\overline{\delta}_iy_i\quad\Rightarrow\quad\hat{\lambda}_1=\frac{\sum_{i=1}^n\overline{\delta}_i}{\sum_{j=1}^n\overline{\delta}_jy_j}
    \end{align*}
    and finally,
    \begin{align*}
        \frac{\partial Q(\theta|\theta^\prime)}{\partial\lambda_2}=\sum_{i=1}^n\frac{1-\overline{\delta}_i}{\lambda_2}-(1-\overline{\delta}_i)y_i\quad\Rightarrow\quad\sum_{i=1}^n\frac{1-\overline{\delta}_i}{\hat{\lambda}_2}=\sum_{i=1}^n(1-\overline{\delta}_i)y_i\quad\Rightarrow\quad\hat{\lambda}_2=\frac{\sum_{i=1}^n(1-\overline{\delta}_i)}{\sum_{j=1}^n(1-\overline{\delta}_j)y_j}
    \end{align*}
    where now $(\hat{\alpha},\hat{\lambda}_1,\hat{\lambda}_2)$ maximizes the $Q$ function at each iteration given a previous parameterization $\theta^\prime$.\hfill{$\qed$}
\end{document}