\documentclass[10pt]{article}
\usepackage[margin=1.3cm]{geometry}

% Packages
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{bbm} 
\usepackage{dutchcal} % [dutchcal, calrsfs, pzzcal] calligraphic fonts
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[tracking]{microtype}

% Palatino for text goes well with Euler
\usepackage[sc,osf]{mathpazo}   % With old-style figures and real smallcaps.
\linespread{1.025}              % Palatino leads a little more leading

% Euler for math and numbers
\usepackage[euler-digits,small]{eulervm}

% Command initialization
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\graphicspath{{./images/}}

% Custom Commands
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\bp}[1]{\left({#1}\right)}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\1}[1]{\mathbbm{1}_{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\nck}[2]{{#1\choose#2}}
\newcommand{\pc}[1]{\pazocal{#1}}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand*{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand*{\ceil}[1]{\left\lceil#1\right\rceil}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\sgm}{sgm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}

    \begin{center}
        {\bf\large{MATH 857: STATISTICAL LEARNING II}}
        \smallskip
        \hrule
        \smallskip
        {\bf Assignment} 1\hfill {\bf Connor Braun} \hfill {\bf 2024-02-04}
    \end{center}
    \noindent{\bf Problem 2}\\[5pt]
    Consider a problem of the form 
    \[\min_{\beta\in\mbb{R}^m,\theta\in\mbb{R}^n}(f(\beta)+g(\theta))\quad\text{subject to $A\beta+B\theta=c$}\]
    where $f:\mbb{R}^m\rightarrow\mbb{R}$ and $g:\mbb{R}^n\rightarrow\mbb{R}$ are convex, $A\in\mbb{R}^{d\times m}$ and $B\in\mbb{R}^{d\times n}$ are known matrices
    of constraints, with $c\in\mbb{R}^d$ a constraint vector.\\[5pt]
    Define
    \[L_\rho(\beta,\theta,\mu):=f(\beta)+g(\theta)+\langle\mu, A\beta+B\theta-c\rangle+\tfrac{\rho}{2}\|A\beta+B\theta-c\|^2\]
    where $\rho>0$ is a small, fixed parameter (akin to a learning rate in ML) and $\mu\in\mbb{R}^d$. The Alternating Direction Method of Multipliers (ADMM)
    tries to solve the optimization problem by the following updates:
    \begin{align*}
        \beta^{t+1}&=\argmin_{\beta\in\mbb{R}^m} L_\rho(\beta,\theta^t,\mu^t)\\
        \theta^{t+1}&=\argmin_{\theta\in\mbb{R}^n} L_\rho(\beta^{t+1},\theta,\mu^t)\\
        \mu^{t+1}&=\mu^t+\rho(A\beta^{t+1}+B\theta^{t+1}-c).
    \end{align*}
    Under relatively mild conditions, one can show that this procedure converges to and optimal solution to the above optimization problem.\\[5pt]
    {\bf a)}\hspace{5pt} Assume the design matrix $X$ and response vector $y$ have been standardized. The Lagrange form of the LASSO can be expressed in an equivalent form as
    \[\min_{\beta\in\mbb{R}^P,\theta\in\mbb{R}^p}\left\{\tfrac{1}{2}\|y-X\beta\|^2+\lambda\|\theta\|_1\right\}\quad\text{subject to $\beta-\theta=0$}.\]
    Find explicitly the ADMM updates for this problem.\\[5pt]
    {\bf Proof}\hspace{5pt} For this problem, define
    \[f(\beta)=\frac{1}{2}\|y-X\beta\|^2,\quad g(\theta)=\lambda\|\theta\|_1,\quad A=1,\quad B=-1,\quad c=0\tag{3}\]
    and fix some $\rho\in\mbb{R}$ nonnegative. Now, fix $\beta^0,\theta^0,\mu^0\in\mbb{R}^p$. The ADMM updates allow us to recursively
    define sequences of vectors $\{\beta^t\}_{t\geq 0}$, $\{\theta^t\}_{t\geq 0}$ and $\{\mu^t\}_{t\geq 0}$ by the above updates using our definitions in (3).
    Let us find each of these in turn, beginning with a recursive formula for $\beta^{t+1}$ given $\beta^t$ for some $t\geq 0$.
    \begin{align*}
        \beta^{t+1}&=\argmin_{\beta\in\mbb{R}^p} \left\{f(\beta)+g(\theta^t)+\langle\mu^t, A\beta+B\theta^t-c\rangle+\tfrac{\rho}{2}\|A\beta+B\theta^t-c\|^2\right\}\\
        &=\argmin_{\beta\in\mbb{R}^p} \left\{\frac{1}{2}(y-X\beta)^T(y-X\beta)+\lambda\|\theta^t\|_1+\langle\mu^t,\beta-\theta^t\rangle+\tfrac{\rho}{2}(\beta-\theta^t)^T(\beta-\theta^t)\right\}\\
        &=\argmin_{\beta\in\mbb{R}^p} \left\{\frac{1}{2}(y^Ty-2y^TX\beta+\beta^TX^TX\beta)+(\mu^t)^T\beta-(\mu^t)^T\theta^t+\frac{\rho}{2}(\beta^T\beta-2(\theta^t)^T\beta+(\theta^t)^T(\theta^t))\right\}\\
        &=\argmin_{\beta\in\mbb{R}^p} \left\{-y^TX\beta+\frac{1}{2}\beta^TX^TX\beta+(\mu^t)^T\beta+\frac{\rho}{2}\beta^T\beta-\rho(\theta^t)^T\beta\right\}\\
        &=:\argmin_{\beta\in\mbb{R}^p} \left\{G(\beta)\right\}.
    \end{align*}
We can solve this by identifying critical points in the gradient of $G$. That is, if $\beta^{t+1}$ is to be our update, then it must satisfy
\begin{align*}
    0=\nabla_\beta G(\beta^{t+1})&=-X^Ty+X^TX\beta^{t+1}+\mu^t+\rho\beta^{t+1}-\rho\theta^t
\end{align*}
which implies that
\[(X^TX+\rho\mbb{I})\beta^{t+1}=X^Ty-\mu^t+\rho\theta^t\quad\Rightarrow\quad\beta^{t+1}=(X^TX+\rho\mbb{{I}})^{-1}(X^Ty+\rho\theta^t-\mu^t).\]
where $\mbb{I}$ is the $p\times p$ identity matrix. It can also be shown that $X^TX+\rho\mbb{I}$ is nonsingular since $\rho>0$, but we omit this from the present proof.
We can also see that this point is a global minimum of $G$, since the Hessian is given by
\begin{align*}
    \nabla^2_\beta G(\beta)=X^TX+\rho\mbb{I},\quad\text{and $\forall x\in\mbb{R}^p$,}\quad x^T(X^TX+\rho\mbb{I})x=x^TX^TXx+\rho x^Tx=\|Xx\|^2+\rho\|x\|^2\geq 0.
\end{align*}
so $\nabla^2_\beta G(\beta)$ is positive semidefinite, and thus convex on $\mbb{R}^p$, such that $\beta^{t+1}$ is in fact a global minimum. We may proceed similarly to find the update formula for $\theta^{t+1}$. That is,
\begin{align*}
    \theta^{t+1}&=\argmin_{\theta\in\mbb{R}^p}\left\{\frac{1}{2}(y-X\beta^{t+1})^T(y-X\beta^{t+1})+\lambda\|\theta\|_1+\langle\mu^t,\beta^{t+1}-\theta\rangle+\tfrac{\rho}{2}(\beta^{t+1}-\theta)^T(\beta^{t+1}-\theta)\right\}\\
    &=\argmin_{\theta\in\mbb{R}^p}\left\{\lambda\|\theta\|_1-(\mu^t)^T\theta-\rho(\beta^{t+1})^T\theta+\frac{\rho}{2}\theta^T\theta\right\}\tag{4}\\
    &=:\argmin_{\theta\in\mbb{R}^p}\left\{H(\theta)\right\}
\end{align*}
where now the function $H$ can be decomposed into two convex parts, one differentiable and the other not differentiable, but separable. That is,
\begin{align*}
    H(\theta)&=\frac{\rho}{2}\sum_{i=1}^p\theta_i^2-\sum_{i=1}^p\mu_i^t\theta_i-\rho\sum_{i=1}^p\beta^{t+1}_i\theta_i+\lambda\sum_{j=1}^p|\theta_j|\\
    &=\frac{\rho}{2}\sum_{i=1}^p\theta_i^2-\rho\sum_{i=1}^p\left(\beta_i^{t+1}-\frac{\mu_i}{\rho}\right)\theta_i+\lambda\sum_{j=1}^p|\theta_j|\\
    &=\frac{\rho}{2}\left[\sum_{i=1}^p\theta_i^2-2\bp{\beta_i^{t+1}-\frac{\mu_i}{\rho}}\theta_i\right]+\lambda\sum_{j=1}^p|\theta_j|\\
    &=\frac{1}{2}\left[\sum_{i=1}^p\theta_i^2-2\bp{\beta_i^{t+1}-\frac{\mu_i}{\rho}}\theta_i+\bp{\beta^{t+1}_i+\frac{\mu_i}{\rho}}^2\right]+\frac{\lambda}{\rho}\sum_{j=1}^p|\theta_j|\\
    &=\frac{1}{2}\left[\sum_{i=1}^p\bp{\theta_i-\bp{\beta_i^{t+1}-\frac{\mu_i}{\rho}}}^2\right]+\frac{\lambda}{\rho}\sum_{j=1}^p|\theta_j|\tag{5}\\
    &=\frac{1}{2}\left[\sum_{i\neq j}\bp{\theta_i-\bp{\beta_i^{t+1}-\frac{\mu_i}{\rho}}}^2\right]+\frac{1}{2}\bp{\theta_j-\bp{\beta_j^{t+1}-\frac{\mu_j}{\rho}}}^2+\frac{\lambda}{\rho}|\theta_j|+\frac{\lambda}{\rho}\sum_{i\neq j}|\theta_i|
\end{align*}
where $1\leq j\leq p$ is an arbitrary coordinate. We can clearly verify from (5)that $H$ is of the form mentioned, and so coordinate descent is guaranteed to converge to the global minimum. That is, we can update each coordinate individually as
\begin{align*}
    \theta^{t+1}_j&=\argmin_{\theta_j\in\mbb{R}}\left\{\frac{1}{2}\bp{\theta_j-\bp{\beta_j^{t+1}-\frac{\mu_j}{\rho}}}^2+\frac{\lambda}{\rho}|\theta_j|\right\}
\end{align*}
which we know (from our coverage of LASSO) has a minimizer given by
\[\theta^{t+1}_j=\mathcal{S}_{\frac{\lambda}{\rho}}\bp{\beta_j^{t+1}-\frac{\mu_j}{\rho}}\]
where the $\beta^{t+1}_j+\tfrac{\mu_j}{\rho}$ in the soft-thresholding operator is due to the fact that this is the least squares solution to the differentiable component of this univariate optimization. Since we apply this for every coorindate, we may write
\[\theta^{t+1}=\mathcal{S}_{\frac{\lambda}{\rho}}\bp{\beta^{t+1}-\frac{1}{\rho}\mu}.\]
Finally, the update for $\mu^{t+1}$ follows directly from the definition with no derivation required:
\begin{align*}
    \mu^{t+1}=\mu^t+\rho(\beta^{t+1}-\theta^{t+1})
\end{align*}
and we are done.\hfill{$\qed$}\\[5pt]
\end{document}
