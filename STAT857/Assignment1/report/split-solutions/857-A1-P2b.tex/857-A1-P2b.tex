\documentclass[10pt]{article}
\usepackage[margin=1.3cm]{geometry}

% Packages
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{bbm} 
\usepackage{dutchcal} % [dutchcal, calrsfs, pzzcal] calligraphic fonts
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[tracking]{microtype}

% Palatino for text goes well with Euler
\usepackage[sc,osf]{mathpazo}   % With old-style figures and real smallcaps.
\linespread{1.025}              % Palatino leads a little more leading

% Euler for math and numbers
\usepackage[euler-digits,small]{eulervm}

% Command initialization
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\graphicspath{{./images/}}

% Custom Commands
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\bp}[1]{\left({#1}\right)}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\1}[1]{\mathbbm{1}_{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\nck}[2]{{#1\choose#2}}
\newcommand{\pc}[1]{\pazocal{#1}}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand*{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand*{\ceil}[1]{\left\lceil#1\right\rceil}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\sgm}{sgm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}

    \begin{center}
        {\bf\large{MATH 857: STATISTICAL LEARNING II}}
        \smallskip
        \hrule
        \smallskip
        {\bf Assignment} 1\hfill {\bf Connor Braun} \hfill {\bf 2024-02-04}
    \end{center}
    \noindent{\bf b)} Suppose that instead we write
        \[\min_{\beta\in\mbb{R}^P,\theta\in\mbb{R}^p}\left\{\tfrac{1}{2n}\|y-X\beta\|^2+\lambda\|\theta\|_1\right\}\quad\text{subject to $\beta-\theta=0$}.\]
    From {\bf a}, find the corresponding update formulae for $\beta^{t+1},\theta^{t+1}$ and $\mu^{t+1}$.\\[5pt]
    {\bf Proof}\hspace{5pt} In this new scenario, our definitions for $g$, $A$, $B$ and $c$ are unchanged, and the only modification is that now
    \[f(\beta)=\frac{1}{2n}\|y-X\beta\|^2.\]
    For $\beta^{t+1}$, it is easy to see that the new objective function $G$ is given by
    \begin{align*}
        G(\beta):=-\frac{1}{n}y^TX\beta+\frac{1}{2n}\beta^TX^TX\beta+(\mu^t)^T\beta+\frac{\rho}{2}\beta^T\beta-\rho(\theta^t)^T\beta
    \end{align*}
    identical to what we found previously except for the factor of $\tfrac{1}{n}$ in the first two terms. We can easily compute its gradient
    \begin{align*}
        \nabla_\beta G(\beta)&=-\frac{1}{n}X^Ty+\frac{1}{n}X^TX\beta+\mu^t+\rho\beta-\rho\theta^t
    \end{align*}
    which has a new global minimum given by
    \begin{align*}
        \beta^{t+1}&=\bp{\frac{1}{n}X^TX+\rho\mbb{I}}^{-1}\bp{\frac{1}{n}X^Ty+\rho\theta^t-\mu^t}
    \end{align*}
    which is still a global minimum because the Hessian is
    \begin{align*}
        \nabla^2_\beta G(\beta)&=\frac{1}{n}X^TX+\rho\mbb{I}\quad\text{and, $\forall x\in \mbb{R}^p$,}\quad x^T\frac{1}{n}X^TXx+\rho x^Tx=\frac{1}{n}\|Xx\|^2+\rho\|x\|^2\geq 0
    \end{align*}
    such that $\nabla^2_\beta G$ is positive semidefinite. Now, observe that in (4) of part {\bf a}, the objective function does not include $f$. This means that modifying $f$ does not change the update rule for $\theta^{t+1}$, and this is
    also the case for $\mu^{t+1}$. Thus, their update rules remain unchanged. Thus, altogether our final update formulae are given by
    \begin{align*}
        \beta^{t+1}&=\bp{\frac{1}{n}X^TX+\rho\mbb{I}}^{-1}\bp{\frac{1}{n}X^Ty+\rho\theta^t-\mu^t}\\
        \theta^{t+1}&=\mathcal{S}_{\frac{\lambda}{\rho}}\bp{\beta^{t+1}-\frac{1}{\rho}\mu}\\
        \mu^{t+1}&=\mu^t+\rho(\beta^{t+1}-\theta^{t+1})
    \end{align*}
    and we are done. \hfill{$\qed$}
\end{document}