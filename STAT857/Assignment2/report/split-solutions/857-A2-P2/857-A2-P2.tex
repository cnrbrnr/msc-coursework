\documentclass[10pt]{article}
\usepackage[margin=1.3cm]{geometry}

% Packages
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{bbm} 
\usepackage{dutchcal} % [dutchcal, calrsfs, pzzcal] calligraphic fonts
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[tracking]{microtype}

% Palatino for text goes well with Euler
\usepackage[sc,osf]{mathpazo}   % With old-style figures and real smallcaps.
\linespread{1.025}              % Palatino leads a little more leading

% Euler for math and numbers
\usepackage[euler-digits,small]{eulervm}

% Command initialization
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\graphicspath{{./images/}}

% Custom Commands
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\bp}[1]{\left({#1}\right)}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\1}[1]{\mathbbm{1}_{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\nck}[2]{{#1\choose#2}}
\newcommand{\pc}[1]{\pazocal{#1}}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand*{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand*{\ceil}[1]{\left\lceil#1\right\rceil}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\sgm}{sgm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}
    \begin{center}
        {\bf\large{MATH 857: STATISTICAL LEARNING II}}
        \smallskip
        \hrule
        \smallskip
        {\bf Assignment 2} \hfill {\bf Connor Braun} \hfill {\bf 2024-02-08}
    \end{center}
    \noindent{\bf Problem 2}\\[5pt]
    Show that for a $p\times p$ matrix $A$ not depending on $x\in\mbb{R}^p$,
    \begin{align*}
        \frac{\partial x^TAx}{\partial x} =(A+A^T)x.
    \end{align*}
    {\bf Proof}\hspace{5pt}We may proceed directly. First, let us re-express the quadratic form to make computing the gradient
    easier. For $1\leq i,j\leq p$, let us denote the $i,j$-th element of $A$ with $A_{i,j}$. Then, in excrutiating detail:
    \begin{align*}
        x^TAx=\begin{pmatrix}
            x_1\cdots x_p
        \end{pmatrix}\begin{pmatrix}
            A_{1,1} & A_{1,2} & \cdots & A_{1, p}\\
            A_{2,1} & A_{2,2} & \cdots & A_{2, p}\\
            \vdots & \vdots & \ddots & \vdots\\
            A_{p,1} & A_{p,2} & \cdots & A_{p,p}
        \end{pmatrix}\begin{pmatrix}
            x_1\\
            \vdots\\
            x_p
        \end{pmatrix} &= \begin{pmatrix}
            x_1,\dots x_p
        \end{pmatrix}\begin{pmatrix}
            A_{1,1}x_1+A_{1,2}x_2+\dots +A_{1,p}x_p\\
            A_{2,1}x_1+A_{2,2}x_2+\dots+A_{2,p}x_p\\
            \vdots\\
            A_{p,1}x_1+A_{p,2}x_2+\dots+A_{p,p}x_p
        \end{pmatrix}\\
        &=\sum_{1\leq i,j\leq p}x_jA_{j,i}x_i\\
        &=\sum_{i=1}^px_i^2A_{i,i}+\sum_{j=2}^p\sum_{i<j}x_jA_{j,i}x_i+\sum_{i=2}^p\sum_{j<i}x_jA_{j,i}x_i.
    \end{align*}
    From here, let us find the $k$th element of the gradient with $1\leq k\leq p$.
    \begin{align*}
        \frac{\partial x^TAx}{\partial x_k}&=\frac{\partial}{\partial x_k}\bp{\sum_{i=1}^px_i^2A_{i,i}}+\frac{\partial}{\partial x_k}\bp{\sum_{j=2}^p\sum_{i<j}x_jA_{j,i}x_i}+\frac{\partial}{\partial x_k}\bp{\sum_{i=2}^p\sum_{j<i}x_jA_{j,i}x_i}\\
        &=2x_kA_{k,k}+\sum_{i<k}A_{k,i}x_i+\sum_{k<j}x_jA_{j,k}+\sum_{j<k}x_jA_{j,k}+\sum_{k<i}A_{k,i}x_i\\
        &=\bp{A_{k,k}x_k+\sum_{i<k}A_{k,i}x_i+\sum_{k<i}A_{k,i}x_i}+\bp{A_{k,k}x_k+\sum_{k<j}x_jA_{j,k}+\sum_{j<k}x_jA_{j,k}}\\
        &=\sum_{i=1}^pA_{k,i}x_i+\sum_{j=1}^px_jA_{j,k}.
    \end{align*}
    Thus, the full gradient vector is given by
    \begin{align*}
       \frac{\partial x^TAx}{\partial x}&=\begin{pmatrix}
            \sum_{i=1}^pA_{1,i}x_i+\sum_{j=1}^px_jA_{j,1}\\
            \sum_{i=1}^pA_{2,i}x_i+\sum_{j=1}^px_jA_{j,2}\\
            \vdots \\
            \sum_{i=1}^pA_{p,i}x_i+\sum_{j=1}^px_jA_{j,p}
       \end{pmatrix} = Ax+A^Tx=(A+A^T)x
    \end{align*}
    as we intended to show.\hfill{$\qed$}\\[5pt]
\end{document}