\documentclass[10pt]{article}
\usepackage[margin=1.3cm]{geometry}

% Packages
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{bbm} 
\usepackage{dutchcal} % [dutchcal, calrsfs, pzzcal] calligraphic fonts
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[tracking]{microtype}

% Palatino for text goes well with Euler
\usepackage[sc,osf]{mathpazo}   % With old-style figures and real smallcaps.
\linespread{1.025}              % Palatino leads a little more leading

% Euler for math and numbers
\usepackage[euler-digits,small]{eulervm}

% Command initialization
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\graphicspath{{./images/}}

% Custom Commands
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\bp}[1]{\left({#1}\right)}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\1}[1]{\mathbbm{1}_{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\nck}[2]{{#1\choose#2}}
\newcommand{\pc}[1]{\pazocal{#1}}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand*{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand*{\ceil}[1]{\left\lceil#1\right\rceil}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\sgm}{sgm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}
    \begin{center}
        {\bf\large{MATH 857: STATISTICAL LEARNING II}}
        \smallskip
        \hrule
        \smallskip
        {\bf Assignment 2} \hfill {\bf Connor Braun} \hfill {\bf 2024-02-08}
    \end{center}
    {\bf Problem 3}\\[5pt]
    Let $W$ be a $p\times p$ symmetric matrix. Solve
    \begin{align*}
        \min_{\beta\in\mbb{R}^p}\left\{\sum_{i=1}^n(y_i-x_i^T\beta)^2+\beta^TW\beta\right\}.
    \end{align*}
    {\bf Solution}\hspace{5pt} Let $G(\beta):=\sum_{i=1}^n(y_i-x_i^T\beta)^2+\beta^TW\beta$ be the objective function. To minimize it, we identify it's (in this case unique) critical point, and use the Hessian to characterize the nature of this extrema. We can rewrite this function as 
    \begin{align*}
        G(\beta)&=(y-X\beta)^T(y-X\beta)+\beta^T W\beta=y^Ty-2y^TX\beta+\beta^TX^TX\beta+\beta^TW\beta
    \end{align*}
    with $X$ the design matrix and $y=(y_1,\dots y_n)^T$. Then, the gradient is simply
    \begin{align*}
        \frac{\partial G(\beta)}{\partial \beta}&=-2X^Ty+(X^TX+(X^TX)^T)\beta+(W+W^T)\beta\tag{by the previous problem}\\
        &=-2X^Ty+2X^TX\beta+2W\beta\tag{since $W^T=W$}
    \end{align*}
    so that if $\hat{\beta}\in\mbb{R}^p$ is a critical point of $G$, then
    \begin{align*}
        0=\frac{\partial G(\beta)}{\partial\beta}\bigg|_{\hat{\beta}}&=-2X^Ty+2X^TX\hat{\beta}+2W\hat{\beta}\quad\Rightarrow\quad X^TX\hat{\beta}+W\hat{\beta}=X^Ty\quad\Rightarrow\quad(X^TX+W)\hat{\beta}=X^Ty\quad\Rightarrow\quad\hat{\beta}=(X^TX+W)^{-1}X^Ty
    \end{align*}
    provided $X^TX+W$ is nonsingular. Now, computing the Hessian matrix
    \begin{align*}
        \frac{\partial^2G(\beta)}{\partial\beta\partial\beta^T}&=2(X^TX+W)
    \end{align*}
    we have that for $u\in\mbb{R}^p$, $u\neq 0$
    \[2u^T(X^TX+W)u=2(u^TX^TXu+u^TWu)=2((Xu)^TXu+u^TWu)=2(\|Xu\|^2+u^TWu)\geq u^TWu\]
    which is non-negative for arbitrary such $u$ if and only if $W$ is positive semi-definite. Thus, $G$ is convex and $\hat{\beta}$ is the unique global minimum if and only if $W$ is positive semi-definite.
    Taking this for granted, we further can state that $(X^TX+W)$ is nonsingular if either $X^TX$ or $W$ is strictly positive definite (such that the eigenvalues of $X^TX+W$ are all positive, and in particular nonzero).\hfill{$\qed$}\\[5pt]
\end{document}