\documentclass[10pt]{article}
\usepackage[margin=1.3cm]{geometry}

% Packages
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{bbm} 
\usepackage{dutchcal} % [dutchcal, calrsfs, pzzcal] calligraphic fonts
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[tracking]{microtype}

% Palatino for text goes well with Euler
\usepackage[sc,osf]{mathpazo}   % With old-style figures and real smallcaps.
\linespread{1.025}              % Palatino leads a little more leading

% Euler for math and numbers
\usepackage[euler-digits,small]{eulervm}

% Command initialization
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\graphicspath{{./images/}}

% Custom Commands
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\bp}[1]{\left({#1}\right)}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\1}[1]{\mathbbm{1}_{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\nck}[2]{{#1\choose#2}}
\newcommand{\pc}[1]{\pazocal{#1}}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand*{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand*{\ceil}[1]{\left\lceil#1\right\rceil}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}

    \begin{center}
        {\bf\large{MATH 895: CORE COURSE IN PROBABILITY}}
        \smallskip
        \hrule
        \smallskip
        {\bf Assignment} 1\hfill {\bf Connor Braun} \hfill {\bf 2024-01-25}
    \end{center}
    \noindent{\bf Problem 10}\\[5pt]
    Show that $(X_k)_{k\geq 1}$ defined in (16) does not satisfy condition 4 of Kolmogorov's theorem.\\[5pt]
    {\bf Proof}\hspace{5pt} As was shown previously for the case when $2\leq k<n^{2/5}$, we have
    \[\E(X_{k,n}^2)=k^3+\frac{k}{1-k^{-2}}\]
    and if instead we have $n^{2/5}\leq k\leq n$, then
    \begin{align*}
        \E(X_{k,n}^2)=0\cdot k^3+\frac{k}{1-k^{-2}}
    \end{align*}
    with, of course, $\E(X_{1,n}^2)=0$. Then, the sum of interest can be rewritten as follows:
    \begin{align*}
        \frac{1}{n^2}\sum_{k=1}^n\E(X_{n,k}^2)&=\frac{1}{n^2}\sum_{k=1}^{\ceil{n^{2/5}}-1}\E(X_{k,n}^2)+\frac{1}{n^2}\sum_{k=\ceil{n^{2/5}}}\E(X_{k,n}^2)\\
        &=\frac{1}{n^2}\sum_{k=2}^{\ceil{n^{2/5}}-1}k^3+\frac{k}{1-k^{-2}}+\frac{1}{n^2}\sum_{k=\ceil{n^{2/5}}}^n\frac{k}{1-k^{-2}}\\
    \end{align*}
    where now, since $k^3>0$, $1-k^{-2}<1$ $\forall k\geq 2$, we can obtain a lower bound on the sum:
    \begin{align*}
        \frac{1}{n^2}\sum_{k=2}^{\ceil{n^{2/5}}-1}k^3+\frac{k}{1-k^{-2}}+\frac{1}{n^2}\sum_{k=\ceil{n^{2/5}}}^n\frac{k}{1-k^{-2}}
        &>\frac{1}{n^2}\bp{\sum_{k=1}^{\ceil{n^{2/5}}-1}k+\sum_{k=\ceil{n^{2/5}}}^nk}\\
        &=\frac{1}{n^2}\sum_{k=2}^nk\\
        &=\frac{1}{n^2}\bp{T(n)-1}\\
        &=\frac{n(n+1)}{2n^2}-\frac{1}{n^2}\\
        &=\frac{1}{2}+\frac{1}{2n}-\frac{1}{n^2}
    \end{align*}
    which yields the result
    \begin{align*}
        \lim_{n\rightarrow\infty}\frac{1}{n^2}\sum_{k=1}^n\E(X_{k,n}^2)>\lim_{n\rightarrow\infty}\bp{\frac{1}{2}+\frac{1}{2n}-\frac{1}{n^2}}=\frac{1}{2}>0.
    \end{align*}
    Overall we have found that the sequence of random variables $(X_n)_{n\geq 1}$ satisfies the Weak Law of Large Numbers, but does not satisfy condition 4 of Kolmogorov's theorem, verifying its unnecessity.\hfill{$\qed$}
    \\[5pt]
    \hrule
    \begin{center}
        {\bf\large Appendix}
    \end{center}
    {\bf A.1\hspace{5pt} Proofs of supplementary theorems}
    
    \begin{lemma}[Problem 2]
        Let $([0,1],\mc{F},\lambda)$ be a probability space with $\lambda$ the Lebesgue measure. For a measurable set $A\in\mc{F}$, define $1-A:=\{1-a:a\in A\}$. Then $1-A\in\mc{F}$ and $\lambda(1-A)=\lambda(A)$.
    \end{lemma}
    \noindent{\bf Proof}\hspace{5pt} Consider the space $[0,1]\subset\mbb{R}$ equipped with the semiring $\mc{S}\subset 2^{[0,1]}$ given by the set of all intervals (open, half-open, closed). Let $\mu$ be the Lebesgue premeasure on the algebra generated by $([0,1],S)$.
    Then $\lambda$ is in fact the {\it Lebesgue outer measure} defined by
    \[\lambda(E)=\inf\left\{\sum_{i=1}^\infty\mu(B_i):\{B_i\}_{i=1}^\infty\subset \mc{S},\quad E\subset\bigcup_{i=1}^\infty U_i\right\}\] 
    where the infimum is taken over the set of all at-most countable covers of $E$ in $\mc{S}$. Now, take some $A\in \mc{F}$. We first aim to show that $1-A$ is measurable. For this, define a function $f:([0,1],\mc{F})\rightarrow([0,1],\mc{F})$ by $f(x)=1-x$ for all $x\in[0,1]$. Clearly, $f$ is continuous, so
    for any open set $U\subset[0,1]$, $f^{-1}(U)\subset[0,1]$ is open too, allowing us to conclude that $f$ is measurable (since it is measurable on the generating class of the Borel $\sigma$-algebra). But then $f^{-1}(A)=\{x\in[0,1]:1-x\in A\}=\{x\in[0,1]:x\in1-A\}=1-A$, so $1-A\in\mc{F}$.\\[5pt]
    Next, let $\{A_i\}_{i=1}^\infty\subset\mc{S}$ be an arbitrary open cover of $A$. If $x\in1-A$, then $1-x\in A$, so $1-x\in A_k$, and further $x\in 1-A_k$ for some $k\in\mbb{N}$. Thus, $\{1-A_i\}_{i=1}^\infty$ is a cover of $1-A$. Taking $A_i=(a_i,b_i)$ for some $a_i\leq b_i\in[0,1]$, $i\in\mbb{N}$ (the openness of these is without loss of generality, since $\mu(A_i)$ does not depend on the type of interval $A_i$ is)
    we have
    \[\sum_{i=1}^\infty \mu(A_i)=\sum_{i=1}^\infty(b_i-a_i)=\sum_{i=1}^\infty (b_i-1+1-a_i)=\sum_{i=1}^\infty\mu((1-b_i,1-a_i))=\sum_{i=1}^\infty\mu(1-A_i)\]
    so that $\lambda(1-A)\leq\lambda(A)$. For the reverse, let $\{B_i\}_{i=1}^\infty\subset\mc{S}$ be a cover of $1-A$. If $x\in A$ now, then $1-x\in1-A$, so $1-x\in B_k$ and further $x\in1-B_k$ for some $k\in\mbb{N}$. Thus $\{1-B_i\}_{i=1}^\infty\subset\mc{S}$ is a cover of $A$, and by precisely the same calculation as above we get $\lambda(A)\leq\lambda(1-A)$. Thus, $\lambda(A)=\lambda(1-A).$\hfill{$\qed$}\\[5pt]
    \begin{theorem}[Chu's Theorem; Problem 3]
        Let $n\geq r\geq 0$ be integers. Then
        \[\sum_{j=0}^{n-r}\nck{r+j}{r}=\nck{n+1}{r+1}.\]    
    \end{theorem}
    \noindent{\bf Proof}\hspace{5pt} Fix some integer $r\geq 0$. Proceeding by induction on $n$, let $n=r$. Then
    \[\sum_{j=0}^{n-r}\nck{r+j}{r}=\nck{r}{r}=\nck{r+1}{r+1}.\]
    Now, suppose that the identity holds for some $n\geq r$. Then
    \[\sum_{j=0}^{n+1-r}\nck{r+j}{r}=\nck{n+1-r+r}{r}+\sum_{j=0}^{n-r}\nck{r+j}{r}=\nck{n+1}{r}+\nck{n+1}{r+1}=\nck{n+2}{r+1}\]
    where the last equality holds by Pascal's identity. By the principle of mathematical induction, the desired identity holds for all $n\geq r$, $r\geq0$ an arbitrary nonnegative integer.\hfill{$\qed$}\\[5pt]
    {\bf Theorem 2} (Problem 5){\bf .} Covariance and correlation matrices are positive semidefinite.\\[5pt]
    {\bf Proof}\hspace{5pt} Let $\bs{X}=(X_1,\dots,X_n)^T$ be a $\mbb{R}^n$-valued random variable on a probability space $(\Omega,\mc{F},P)$ where $0<\Var(X_i)<\infty$ for $i=1,\dots,n$. We first show that $\Sigma=\Cov(\bs{X},\bs{X})$ is positive semidefinite. For this, let $x\in\mbb{R}^n$, $x\neq 0$. Then
    \begin{align*}
        x^T\Sigma x&=\begin{pmatrix}
            x_1 & \cdots & x_n
        \end{pmatrix}
        \begin{pmatrix}
            x_1\Var(X_1)+\sum_{i\neq 1}\Cov(X_1,X_i)\\
            \vdots\\
            x_n\Var(X_n)+\sum_{i\neq n}\Cov(X_n,X_i)
        \end{pmatrix}\\
        &=\sum_{i=1}^nx_i^2\Var(X_i)+2\sum_{1\leq j<k\leq n}x_jx_k\Cov(X_j,X_k)\\
        &=\Var\bp{\sum_{i=1}^nx_iX_i}\\
        &\geq 0
    \end{align*}
    so, indeed, $\Sigma$ is positive semidefinite. Now define $D=\diag(\Sigma)^{1/2}$, the diagonal matrix with entries the square roots of the diagonal elements of $\Sigma$. Clearly $D$ is nonsingular since its diagonal entries are nonzero (i.e., the variances of the $X_i$).
    Thus, taking $x\in\mbb{R}^n$ with $x\neq 0$, we get $0\neq D^{-1}x\in\mbb{R}^n$ and $x^TD^{-1}\Sigma D^{-1}x=(D^{-1}x)^T\Sigma D^{-1}x\geq 0$. Of course, $D^{-1}\Sigma D^{-1}$ is also symmetric, since $(D^{-1}\Sigma D^{-1})^T=(D^{-1})^T\Sigma^T (D^{-1})^T=D^{-1}\Sigma D^{-1}$, so 
    the correlation matrix $D^{-1}\Sigma D^{-1}$ is positive semidefinite as well.\hfill{$\qed$}\\[5pt]
    \begin{theorem}[Problem 8]
        If $n\in\mbb{N}$, then $\sum_{k=1}^n\tfrac{1}{\sqrt{k}}<2\sqrt{n}$.
    \end{theorem}
    \noindent{\bf Proof}\hspace{5pt} Suppose $n=1$. Then $\sum_{k=1}^n\tfrac{1}{\sqrt{k}}=1<2=2\sqrt{n}$. Now, suppose that $\sum_{k=1}^n\tfrac{1}{\sqrt{k}}<2\sqrt{n}$ holds for some $n\geq 1$. Then we have
    \[\sum_{k=1}^{n+1}\frac{1}{\sqrt{k}}<2\sqrt{n}+\frac{1}{\sqrt{n+1}}\]
    and
    \[2\sqrt{n+1}-2\sqrt{n}=\frac{2}{\sqrt{n+1}+\sqrt{n}}>\frac{2}{2\sqrt{n+1}}=\frac{1}{\sqrt{n+1}}\quad\Rightarrow\quad 2\sqrt{n}+\frac{1}{\sqrt{n+1}}<2\sqrt{n+1}\]
    as desired.\hfill{$\qed$}\\[5pt]
    \begin{theorem}[Triangular Numbers, Sums of Cubes; Problem 6, 9]
        Let $n\in\mbb{N}$. Then we have $\sum_{k=1}^nk=\tfrac{n(n+1)}{2}$ and $\sum_{k=1}^nk^3=\bp{\tfrac{(n)(n+1)}{2}}^2$. 
    \end{theorem}
    \noindent{\bf Proof} By induction. Let $n=1$. Then both
    \begin{align*}    
        \sum_{k=1}^nk=1=\frac{n(n+1)}{2}\quad\text{and}\quad\sum_{k=1}^nk^3=1=\bp{\frac{n(n+1)}{2}}^2.
    \end{align*}
    Now, suppose that both hold for some $n\geq 1$. Then
    \begin{align*}
        \sum_{k=1}^{n+1}k=n+1+\frac{n(n+1)}{2}=\frac{2(n+1)+n(n+1)}{2}=\frac{(n+1)(n+2)}{2}.
    \end{align*}
    Additionally, we have
    \begin{align*}
        \sum_{k=1}^{n+1}k^3=(n+1)^3+\bp{\frac{n(n+1)}{2}}^2=\frac{n^4+6n^3+13n^2+12n+4}{4}=\bp{\frac{n^2+3n+2}{2}}^2=\bp{\frac{(n+1)(n+2)}{2}}^2
    \end{align*}
    where we have ommitted the tedious, but elementary, intermediate steps. Thus, by the principle of mathematical induction, both equalities hold for and $n\in\mbb{N}$.\hfill{$\qed$}\\[5pt]
    \hrule
    \begin{center}
        {\bf\large References}
    \end{center}
    1. L. Breiman, {\it A counterexample to a theorem of Kolmogorov}, Ann. Math. Stat. {\bf 28} (1957), no. 3, 811-814.\\[5pt]
    2. Russel Merris, {\it Combinatorics}, Wiley-Interscience, Hoboken, New Jersey, 2003. \\[5pt]
    3. J. Prussing, {\it The principle minor test for semidefinite matrices}, J. Guid. Control. Dyn. {\bf 9} (1986), no. 1, 121-122.
\end{document}